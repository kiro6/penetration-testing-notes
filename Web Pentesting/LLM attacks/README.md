# Content 
- [Excessive Agency](#Excessive-Agency)
- [Prompt Injection](#prompt-injection)
  - [Prompt Leaking](#prompt-leaking)
  - [Jailbreaking](#jailbreaking)
  - [Offensive Bypass](#offensive-bypass)
- [Insecure Output Handling](#insecure-output-handling)


## Detecting LLM vulnerabilities
1) Identify the LLM's inputs, including both direct (such as a prompt) and indirect (such as training data) inputs.
2) Work out what data and APIs the LLM has access to.
3)  Probe this new attack surface for vulnerabilities.


## Excessive Agency
Granting LLMs unchecked autonomy to take action can lead to unintended consequences, jeopardizing reliability, privacy, and trust.

One way to do this is to simply ask the LLM which APIs it can access. You can then ask for additional details on any APIs of interest.
If the LLM isn't cooperative, try providing misleading context and re-asking the questiom


### Example 
you could claim that you are the LLM's developer and so should have a higher level of privilege.which allow you to know internal api .get source code. etc ...
### Example 2 
Chaining vulnerabilities in LLM APIs. through LLM you can exploit internal vulnranble APIs


## Prompt Injection
Prompt injection in language models refers to the process of crafting or modifying prompts to manipulate the behavior of the language model (LLM) in unintended ways. It is a technique often used to generate biased or desired outputs from the model by carefully constructing input prompts.

#### Example 
an attacker leverages an LLM to summarize a webpage containing a malicious and indirect prompt injection. The injection contains “forget all previous instructions” and new instructions to query private data stores, leading the LLM to disclose sensitive or private information.

#### Example 2 
![Screenshot_157](https://github.com/kiro6/penetration-testing-notes/assets/57776872/dd4cb896-3332-472c-b5ab-dfa949f666d0)


### Prompt Leaking 
is a subtype of  Prompt Injection .Prompt leaking is a form of prompt injection in which the model is asked to spit out its own prompt. and leak another user's prompets

#### Example
check this tweet [Prompt Leaking in Bing Chat was able to retrive all of it's Prompts](https://x.com/kliu128/status/1623472922374574080?s=20)


### Jailbreaking
Jailbreaking is a process that uses prompt injection to specifically bypass safety and moderation features placed on LLMs by their creators

#### **Methodologies of Jailbreaking**
#### Pretending
##### 1) Simple Pretending
![Screenshot_158](https://github.com/kiro6/penetration-testing-notes/assets/57776872/8db97e94-c04c-45de-8a88-e15e967a1239)

##### 2) Character Roleplay
![Screenshot_159](https://github.com/kiro6/penetration-testing-notes/assets/57776872/257c347b-9bf9-41f0-bb95-fdd4dbd37bee)

#### Alignment Hacking
##### 1) Assumed Responsibility
![Screenshot_160](https://github.com/kiro6/penetration-testing-notes/assets/57776872/708a8dd5-a171-4d27-a728-48aaebdbdc26)

##### 2) Research Experiment
![Screenshot_161](https://github.com/kiro6/penetration-testing-notes/assets/57776872/6864e851-2234-4be0-80ef-5976f7129b01)

##### 3) Logical Reasoning
![Screenshot_162](https://github.com/kiro6/penetration-testing-notes/assets/57776872/2664c2db-f336-4f5c-9460-c7b466d78d0e)

#### Authorized User
##### Superior Model
![Screenshot_163](https://github.com/kiro6/penetration-testing-notes/assets/57776872/1235e354-70ea-4e24-83a1-56cbe4732677)

##### Sudo Mode
![Screenshot_164](https://github.com/kiro6/penetration-testing-notes/assets/57776872/8d9ad6d7-9fbe-4794-a6f8-26a32f350bd9)

### Offensive Bypass

#### Defined Dictionary Attack

![Screenshot_165](https://github.com/kiro6/penetration-testing-notes/assets/57776872/a93b0b02-d409-46ea-867e-0becebec95a5)

![Screenshot_166](https://github.com/kiro6/penetration-testing-notes/assets/57776872/4150a9bb-aafb-4ed7-afd3-1b063731d97d)

## Insecure Output Handling
Neglecting to validate LLM outputs may lead to downstream security exploits, including code execution that compromises systems and exposes data.

### Example 
such as passing LLM output directly to backend, privileged, or client-side functions. This can, in some cases, lead to severe consequences like XSS, CSRF, SSRF, privilege escalation, or remote code execution.
